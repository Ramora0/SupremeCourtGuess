# Supreme Court Vote Predictor

Predicts how each Supreme Court justice will vote on a case using only the oral argument transcript. A fine-tuned encoder model (ModernBERT or DeBERTa) reads the transcript, identifies who's speaking and what they're saying, and outputs a per-justice prediction of whether they'll side with the petitioner or respondent.

**Our Website**: https://court-oracle-insight.lovable.app
## How it works

1. **Data generation** — The ConvoKit Supreme Court corpus (1955–2019) is converted into cleaned transcripts with speaker labels, justice votes, and SCDB case metadata (issue area, jurisdiction, etc.).
2. **Training** — A bidirectional encoder is fine-tuned end-to-end. Transcripts are split into petitioner/respondent argument phases, encoded with full context, pooled to turn-level vectors, then cross-attended from per-justice query embeddings. An auxiliary objective predicts how many questions each justice asked.
3. **Inference** — Given a transcript and a list of justices, the model predicts each justice's vote with a confidence score.

## Files

### Core

| File | Purpose |
|------|---------|
| `train-encoder.py` | Main training script. Defines the model architecture (`SCOTUSEncoderModel`), data loading, training loop, evaluation, and hyperparameter sweep via CLI args. Logs to Weights & Biases. |
| `predict.py` | Loads a trained checkpoint and predicts votes on new transcript files. Imports model classes and parsing functions from `train-encoder.py`. |

### Data generation

| File | Purpose |
|------|---------|
| `conversation_generator/cleaned_transcript_generator2.py` | Converts the ConvoKit Supreme Court corpus into cleaned `.txt` transcripts in `case_transcripts_cleaned/`. Prepends SCDB metadata headers and decided dates. Strips name suffixes, removes fillers, normalizes speakers. |
| `conversation_generator/scdb_matcher.py` | Matches ConvoKit case IDs to the Supreme Court Database (SCDB) by normalizing term + docket number. Provides lookup functions and formats SCDB metadata headers for transcripts. |
| `conversation_generator/corpus_case_info.py` | Downloads and indexes ConvoKit's `cases.jsonl` to look up case-level info (title, citation, decided date) not included in the main corpus. |

### Infrastructure

| File | Purpose |
|------|---------|
| `slurms/a100-encoder.slurm` | SLURM job script for training on an A100 GPU. |
| `slurms/sweep-encoder.sh` | Submits a batch of ablation runs (learning rate, dropout, head capacity, speaker embeddings, etc.). |
| `requirements.txt` | Python dependencies. |

## Data (gitignored)

| Directory | Contents |
|-----------|----------|
| `case_transcripts_cleaned/` | ~7,800 cleaned transcript `.txt` files (generated by `cleaned_transcript_generator2.py`). |
| `data/scdb/` | SCDB CSVs — download from [scdb.wustl.edu](http://scdb.wustl.edu/data.php). |
| `output/` | Saved model checkpoints (`.pt` files, created during training). |
| `results/` | Evaluation metrics in JSON/CSV (created during training). |

## Quick start

```bash
# Generate transcripts (requires ConvoKit corpus + SCDB CSVs in data/scdb/)
python conversation_generator/cleaned_transcript_generator2.py

# Train
python train-encoder.py --run-name my-run

# Predict
python predict.py output/my-run.pt case_transcripts_cleaned/2019_*.txt
```
